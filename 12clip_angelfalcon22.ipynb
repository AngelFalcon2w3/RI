{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "927ac9f383d0d6b7"
   },
   "source": [
    "# Ejercicio 12: Multimodal Embeddings\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "El objetivo de este ejercicio es observar cómo modelos multimodales como CLIP llevan texto e imágenes al mismo espacio vectorial, y verificar graficando embeddings en 2D.\n",
    "\n",
    "### Pasos:\n",
    "\n",
    "1. Obtener embeddings de imágenes y textos con CLIP.\n",
    "2. Mostrar que ambos viven en el mismo espacio (misma dimensión y comparables).\n",
    "3. Proyectar los vectores a 2D (PCA / t-SNE / UMAP) y graficarlos en un plano.\n",
    "4. Verificar emparejamientos usando cosine similarity (búsqueda texto→imagen e imagen→texto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPEF79jMtegw",
    "outputId": "1e96e774-5a69-4b34-e60b-9c7c735ecd91"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import glob\n",
    "import clip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rj_mj_TE6OHi",
    "outputId": "f970f854-2667-4652-f692-5a518b6a07cd"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvJM8yD47Tsf",
    "outputId": "0c0419e0-ec58-45af-8aeb-98072a53e2de"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446,
     "referenced_widgets": [
      "a6dc953d404443f588200e9bd4902231",
      "823ee9c3def346e2b4bd0bbeb9bcd38a",
      "aa2a8b4d43404822bdbb7a62fb4d1cba",
      "08dc9524d7584f7da6bcedfe7d96a28f",
      "88cb9012af1448dc859b8c33bfe0d806",
      "b352620e1f234041a47416dcb63b37bf",
      "18ee72b2e6ba4006b00ba6d36fd18770",
      "e14e1e7bb8cd4bba83cf14d77ed903c2",
      "342d3d5b269e420fa3822fa62da50a07",
      "dd4cccc91b7c4c6687d22ae0678aae73",
      "7852449315b84634b992ce5792080a66",
      "0979685a3e3447f0937d42e99dd0e4e4",
      "ee1c502195e042bcad5e7d791608cf90",
      "05afa54467c94823a5fdd2d1abc7d0ad",
      "b33711589d384c36aadc2d520851bd03",
      "0f508e59e7ba46959a24c0eb7b1a6423",
      "29ad35b6699a498a90066986e5574afe",
      "a35244dd99db4f7dbcb4eeada788f428",
      "e5bd2a6c4c9346dfaa9bb376a6664e3d",
      "4ffc145164e946e3b88c814d829c83b1",
      "5a178814d42240da903a1dc99ebc34f9",
      "54c7735cbb444e35995e2dcaefe993c8",
      "8a6f4618f4a240dfae8d9fec2c35cebc",
      "2cb2864d3ffb4746836bed42800ce94a",
      "f193b35e51f4400288440f87f8c31b48",
      "212c30346c7b411eb705d58c16067651",
      "8d56892381ec497cb227ff52a7e5cce1",
      "9814c848ca134379ab9590fc040fe715",
      "89b2e293d8dc45d59add614ef79586bb",
      "3334845bdd404efdaa8307e671aa3882",
      "122d09228bbc4666b152e4196c21ef93",
      "4971e34234a04f5d8ae14b0240586c46",
      "2e9b73abbc014c5c9602e5dde9f21345",
      "4378360437eb439c844ced5478659e24",
      "51131c10ae7a4b62accaaf7b1a6c788f",
      "440a326a0e54451a89edb474541f3a56",
      "d273a0ede2da407299327b268ac72965",
      "1db8b96f355d47049bde27a6c489e645",
      "46bfb741d3b848c2b65fe6b3732312b0",
      "29eb9b6724ec4f9f8e1a503ecec7373a",
      "afc34c8b033f42268691cddd75a34159",
      "8a08b750c9014e2d93f017573e7a617e",
      "24ae6855ebf24bedb646de0b5fc7a318",
      "7ee5f5a372914684b1486f6aee0285f3",
      "27cd4695dcc54a1585ab5bf8267f61f4",
      "1a7ff01eb85f4d56aa2446f8c1b65e20",
      "4f9e0abc832244e1a357ae4fc4d6d161",
      "00aef712b332417c927aeb379c8fe1e1",
      "fb2f6f62bf3145aa9150b6da72bd2939",
      "72052ad059604dd98bffb1c9dde24459",
      "2d5a93e74dc1420f9e23b4e85572cc49",
      "9e958e4756cd41cdb8e52c93cde4dc28",
      "4bf82c0a8d624f57bcdb23f88268d848",
      "c492610a1025462ca3672cc04b173720",
      "4edbb65ce13642c8aee36718a9def189",
      "db2c7089b47841c8b37a50953b85660b",
      "d0c710901f66434193103560e1e9a07d",
      "ac28edcdfeb6434e85aa0aa326d94984",
      "4f6a78a3c66a4fff974f7176f2c3d51a",
      "fff56049f55c48f48fe38d71cbbba2c8",
      "5cd6eae0179b4531a1afa0f7eb03ddd0",
      "f901bbbd2d97483ba05304c0807fb04a",
      "73aeeb7f0f9d411a82a59cec2bfb8787",
      "10c7e4b19daa45b9866756645500150e",
      "ab3f3932f48f4a9bbab97ed3206666dc",
      "01694e092e2040e8a28e62a0b062eb86",
      "1420a7fb11314c23b99fe354e9ba0ee7",
      "c01eb390d031449b84e5cbee3df1d24a",
      "d9eca37057f941f3914788417aff24d4",
      "346113922d444e5bbec07f92be4e7049",
      "57d71dedcbe445c096e3a9fdf24f1d51",
      "390a9b12d7684eb38e82cb637611c4b9",
      "fd57843d64864f2988fea854d6ec1c14",
      "2cbf78c18c7c4968aa09efaeeb9eddbe",
      "eef4f8c9f14749979fca26fada13117b",
      "97dd8c558e574b3bb3564fc145883d47",
      "efabfe80c61e452fabac2aea6836b932",
      "71d826eec3f549f1abb4bd8e10cb8979",
      "071d14aad8d34d79915177843167cbe9",
      "0648f104b2ee4643997535eaf673bd05",
      "e683eb73bcae4ab5968256a37f95d74f",
      "3de887bde0c644749b285a6c7283059d",
      "62089959f3404279a684ee7534ca178d",
      "b5b6b0ced02249259ebe2764a5689362",
      "9df31b49efb44229955921e1b01872f6",
      "cb5d21b7d429475e8c78eef116e5372f",
      "72a25a83a1864fb48ed658a2b55e17be",
      "a22b3e7da48a4e2c814ebf2fef4a911e",
      "c0998f89a2894f06874a2638bc2e0277",
      "684b2196162b4cab88f54eaaafa616ae",
      "4daba4537556437a92b2954048b41dfd",
      "8426083ba2aa4d8a9c4d6f9497654e46",
      "bbc9ebf11387410798dc935977f471f1",
      "1086f6c3ebfc40ea836b60782a089984",
      "23e05e51cd234a339dc6a24f8b69fcf4",
      "34a96c76442d49448c242a82fe5bc47a",
      "4c226bbef08b409aa0f2eeb7bd011f3b",
      "bbbc0a3028f24df8a1f5006ef7a4ab24",
      "1205e137ff904718a884cfc983a8e16e"
     ]
    },
    "id": "F_bar66_878G",
    "outputId": "f82f2eaf-e394-4fe9-a862-073612c90c8a"
   },
   "outputs": [],
   "source": [
    "# CARGAR MODELO CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkD1f7xr7zrF",
    "outputId": "7c28e2d1-b89a-457e-88f1-25e912314c3e"
   },
   "outputs": [],
   "source": [
    "# 1. CARGAR IMÁGENES\n",
    "CARPETA_IMAGENES = \"/content/drive/MyDrive/RI/RI\"\n",
    "\n",
    "rutas_imagenes = sorted(glob.glob(os.path.join(CARPETA_IMAGENES, \"*.jpg\")))\n",
    "imagenes = [Image.open(ruta).convert(\"RGB\") for ruta in rutas_imagenes]\n",
    "\n",
    "print(f\"Imágenes cargadas: {len(imagenes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "eIhTH_Wc8WbS"
   },
   "outputs": [],
   "source": [
    "# 2. PREPROCESAR IMÁGENES PARA CLIP\n",
    "imagenes = [\n",
    "    Image.open(p).convert(\"RGB\")\n",
    "    for p in rutas_imagenes\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJavz1698qJj",
    "outputId": "96f19f45-5572-4c8e-c00c-0606e562f147"
   },
   "outputs": [],
   "source": [
    "# 3. OBTENER EMBEDDINGS DE IMÁGENES\n",
    "inputs = processor(images=imagenes, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb_imagenes = model.get_image_features(**inputs)\n",
    "\n",
    "# Normalizar embeddings (norma = 1)\n",
    "emb_imagenes = emb_imagenes / emb_imagenes.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Pasar a numpy (para PCA, t-SNE, cosine similarity)\n",
    "emb_imagenes = emb_imagenes.numpy()\n",
    "\n",
    "print(\"Embeddings de imágenes:\", emb_imagenes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJ7HLjIU9Xgw",
    "outputId": "462e64ec-ffaf-4bec-eda4-0cf16252e480"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"dog\",\n",
    "    \"cat\",\n",
    "    \"car\",\n",
    "    \"bike\",\n",
    "    \"person \",\n",
    "    \"bed\"\n",
    "]\n",
    "\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb_textos = model.get_text_features(**inputs)\n",
    "\n",
    "# Normalizar embeddings\n",
    "emb_textos = emb_textos / emb_textos.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Pasar a numpy (para PCA, t-SNE, cosine similarity)\n",
    "emb_textos = emb_textos.numpy()\n",
    "\n",
    "print(\"Embeddings de texto:\", emb_textos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ADBGg5r9hBh",
    "outputId": "d8b41174-ca23-4450-e89f-d91d61153a5f"
   },
   "outputs": [],
   "source": [
    "print(\"Dimensión imagen:\", emb_imagenes.shape[1])\n",
    "print(\"Dimensión texto:\", emb_textos.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUQ8gY069o7p",
    "outputId": "16f100d0-92d9-4fe2-8d7e-67eb48624d59"
   },
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(\n",
    "    emb_textos,\n",
    "    emb_imagenes\n",
    ")\n",
    "\n",
    "similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "_Rm_nvpO-Q6P"
   },
   "outputs": [],
   "source": [
    "# UNIR EMBEDDINGS (imagen + texto)\n",
    "all_embeddings = np.vstack([emb_imagenes, emb_textos])\n",
    "\n",
    "# PCA 2D\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d_pca = pca.fit_transform(all_embeddings)\n",
    "\n",
    "img_2d_pca = embeddings_2d_pca[:len(emb_imagenes)]\n",
    "txt_2d_pca = embeddings_2d_pca[len(emb_imagenes):]\n",
    "\n",
    "# t-SNE 2D\n",
    "tsne = TSNE(n_components=2, perplexity=12, random_state=42)\n",
    "embeddings_2d_tsne = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "img_2d_tsne = embeddings_2d_tsne[:len(emb_imagenes)]\n",
    "txt_2d_tsne = embeddings_2d_tsne[len(emb_imagenes):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "IBPUh1QZ-bqP",
    "outputId": "0a44e352-c702-4e2e-87d7-6f6e683b2580"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.scatter(img_2d_tsne[:, 0], img_2d_tsne[:, 1], marker='o', label='Imágenes')\n",
    "for i, name in enumerate(rutas_imagenes):\n",
    "    plt.text(\n",
    "        img_2d_tsne[i, 0],\n",
    "        img_2d_tsne[i, 1],\n",
    "        os.path.basename(name),\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.scatter(txt_2d_tsne[:, 0], txt_2d_tsne[:, 1], marker='x', label='Textos')\n",
    "for i, txt in enumerate(texts):\n",
    "    plt.text(\n",
    "        txt_2d_tsne[i, 0],\n",
    "        txt_2d_tsne[i, 1],\n",
    "        txt,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Embeddings Multimodales CLIP (t-SNE 2D)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "zRW45fQh_M7V",
    "outputId": "1ddb6f33-8087-48d0-a5b1-9ef27610775f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.scatter(img_2d_pca[:, 0], img_2d_pca[:, 1], marker='o', label='Imágenes')\n",
    "for i, name in enumerate(rutas_imagenes):\n",
    "    plt.text(\n",
    "        img_2d_pca[i, 0],\n",
    "        img_2d_pca[i, 1],\n",
    "        os.path.basename(name),\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.scatter(txt_2d_pca[:, 0], txt_2d_pca[:, 1], marker='x', label='Textos')\n",
    "for i, txt in enumerate(texts):\n",
    "    plt.text(\n",
    "        txt_2d_pca[i, 0],\n",
    "        txt_2d_pca[i, 1],\n",
    "        txt,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Embeddings Multimodales CLIP (PCA 2D)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "MBlfl6U4_i3e"
   },
   "outputs": [],
   "source": [
    "def buscar_imagen(texto):\n",
    "    # Embedding del texto\n",
    "    inputs = processor(text=[texto], return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        emb_texto = model.get_text_features(**inputs)\n",
    "\n",
    "    # Normalizar\n",
    "    emb_texto = emb_texto / emb_texto.norm(dim=-1, keepdim=True)\n",
    "    emb_texto = emb_texto.numpy()\n",
    "\n",
    "    # Similaridad coseno texto → imágenes\n",
    "    scores = cosine_similarity(\n",
    "        emb_texto,\n",
    "        emb_imagenes\n",
    "    )[0]\n",
    "\n",
    "    idx = np.argmax(scores)\n",
    "\n",
    "    print(\"Texto:\", texto)\n",
    "    print(\"Imagen más similar:\", os.path.basename(rutas_imagenes[idx]))\n",
    "    print(\"Score:\", scores[idx])\n",
    "\n",
    "    # Mostrar imagen\n",
    "    img = Image.open(rutas_imagenes[idx])\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "AHmUF5N__qbf"
   },
   "outputs": [],
   "source": [
    "def buscar_texto(nombre_imagen):\n",
    "    # Cargar imagen\n",
    "    image = Image.open(nombre_imagen).convert(\"RGB\")\n",
    "\n",
    "    # Embedding de la imagen\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        emb_imagen = model.get_image_features(**inputs)\n",
    "\n",
    "    # Normalizar\n",
    "    emb_imagen = emb_imagen / emb_imagen.norm(dim=-1, keepdim=True)\n",
    "    emb_imagen = emb_imagen.numpy()\n",
    "\n",
    "    # Similaridad coseno imagen → textos\n",
    "    scores = cosine_similarity(\n",
    "        emb_imagen,\n",
    "        emb_textos\n",
    "    )[0]\n",
    "\n",
    "    # Mostrar scores ordenados\n",
    "    text_scores = sorted(zip(texts, scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Scores texto:\")\n",
    "    for t, s in text_scores:\n",
    "        print(f\"{t:35s} -> {s:.4f}\")\n",
    "\n",
    "    idx = np.argmax(scores)\n",
    "\n",
    "    print(\"\\nImagen:\", os.path.basename(nombre_imagen))\n",
    "    print(\"Texto más cercano:\", texts[idx])\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "6kaU2PtP_yJo",
    "outputId": "51f4916f-b986-4e49-d8b3-9ef9375c79a0"
   },
   "outputs": [],
   "source": [
    "buscar_imagen(\"person 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "K7AORkQf_95y",
    "outputId": "94f1a934-40e0-44bc-8128-e4da047e67e8"
   },
   "outputs": [],
   "source": [
    "buscar_texto(\"/content/drive/MyDrive/RI/RI/person 1.jpg\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
